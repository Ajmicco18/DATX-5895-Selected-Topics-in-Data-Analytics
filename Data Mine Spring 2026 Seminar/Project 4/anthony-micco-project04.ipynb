{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be02a957-7133-4d02-818e-fedeb3cecb05",
   "metadata": {},
   "source": [
    "# Project 04 -- Anthony Micco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1228853-dd19-4ab2-89e0-0394d7d72de3",
   "metadata": {},
   "source": [
    "**TA Help:** N/A\n",
    "\n",
    "**Collaboration:** N/A\n",
    "\n",
    "**Internet Resources:** \n",
    "- Used Stack Overflow article to understand how to access multiple return values from a function (https://stackoverflow.com/questions/24051509/how-to-return-from-function-nicely-when-multiple-return-values-expected)\n",
    "\n",
    "**ChatGPT, Gemini, Claude, etc:** N/A\n",
    "\n",
    "**Link to AI Chat History**: None\n",
    "\n",
    "**OVERALL MESSAGE:** Any time that you used anything except your brain to solve the questions in these projects, you need to disclose such resources at the start of the project, with details about your usage of the tools.\n",
    "\n",
    "**YOUR OWN WORK:** Even when you utilize other resources, do NOT just copy and paste.  Write all explanations in your own words, using several sentences in English, which are understandable and which you wrote (and did not just copy and paste)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180e742-8e39-4698-98ff-5b00c8cf8ea0",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6962c7f9-2e6d-4a3b-a4fb-3a4f68effdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the current state of the intro-mlops-1 repo\n",
    "import os\n",
    "\n",
    "def print_directory_tree(root_dir, max_depth=3):\n",
    "  root_dir = os.path.abspath(os.path.expanduser(root_dir))\n",
    "\n",
    "  if not os.path.exists(root_dir):\n",
    "    print(f\"Path does not exist: {root_dir}\")\n",
    "    return\n",
    "\n",
    "  for root, dirs, files in os.walk(root_dir):\n",
    "    # Skip hidden directories like .git\n",
    "    dirs[:] = [d for d in dirs if not d.startswith(\".\") and d != (\"__pycache__\")]\n",
    "    files = [f for f in files if not f.startswith(\".\")]\n",
    "\n",
    "    level = root.replace(root_dir, \"\").count(os.sep)\n",
    "    if level >= max_depth:\n",
    "      continue\n",
    "\n",
    "    indent = \" \" * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "\n",
    "    subindent = \" \" * 4 * (level + 1)\n",
    "    for f in files:\n",
    "      print(f\"{subindent}{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49445606-d363-41b4-b479-e319a9a84c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro-mlops-1/\n",
      "    accuracy_plot.png\n",
      "    results.log\n",
      "    analysis.ipynb\n",
      "    README.md\n",
      "    error.log\n",
      "    training_loss_plot.png\n",
      "    requirements.txt\n",
      "    data.csv\n",
      "    main.py\n",
      "    debug.log\n",
      "    training.log\n"
     ]
    }
   ],
   "source": [
    "root_path = \"~/TDM 302/Project04/intro-mlops-1/\"\n",
    "print_directory_tree(root_path, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e08061-e797-4b4a-8e88-e16180368e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (150, 5)\n",
      "Preprocessing data...\n",
      "Training set size: 120\n",
      "Test set size: 30\n",
      "Starting training...\n",
      "Epoch [10/50], Train Loss: 0.7137, Test Loss: 0.6310\n",
      "Epoch [20/50], Train Loss: 0.3657, Test Loss: 0.2785\n",
      "Epoch [30/50], Train Loss: 0.2640, Test Loss: 0.1606\n",
      "Epoch [40/50], Train Loss: 0.1900, Test Loss: 0.1034\n",
      "Epoch [50/50], Train Loss: 0.1588, Test Loss: 0.0696\n",
      "Training completed!\n",
      "Final Test Accuracy:  1.0\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "!python intro-mlops-1/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a03d72-c731-4a6f-8c15-b9c086b6729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro-mlops-1/\n",
      "    README.md\n",
      "    requirements.txt\n",
      "    main.py\n",
      "    plots/\n",
      "        accuracy_plot.png\n",
      "        training_loss_plot.png\n",
      "    notebooks/\n",
      "        analysis.ipynb\n",
      "    logs/\n",
      "        error.log\n",
      "        debug.log\n",
      "        results.log\n",
      "        training.log\n",
      "    data/\n",
      "        data.csv\n",
      "    src/\n"
     ]
    }
   ],
   "source": [
    "#rerunning the print_directory_tree function now that the directory is organized\n",
    "print_directory_tree(root_path, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc601975-35ed-4680-a4e1-0273ee3cc047",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a16336a1-1ef0-41e8-bc7c-49387db27497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro-mlops-1/\n",
      "    README.md\n",
      "    requirements.txt\n",
      "    main.py\n",
      "    plots/\n",
      "        accuracy_plot.png\n",
      "        training_loss_plot.png\n",
      "    notebooks/\n",
      "        analysis.ipynb\n",
      "    logs/\n",
      "        error.log\n",
      "        debug.log\n",
      "        results.log\n",
      "        training.log\n",
      "    data/\n",
      "        data.csv\n",
      "    src/\n",
      "        data_loader.py\n",
      "        __init__.py\n",
      "        visualization.py\n",
      "        neural_net.py\n",
      "        trainer.py\n",
      "        metrics.py\n"
     ]
    }
   ],
   "source": [
    "# getting the updated file structure with print_directory_tree\n",
    "print_directory_tree(root_path, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586edd-ff26-4ce2-8f6b-2424b26f2929",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe0f40d-9655-4653-9ca8-886bdb61cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to ensure the organized model is running properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6eeffa-eb87-49ae-be12-9f575bab1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (150, 5)\n",
      "Preprocessing data...\n",
      "Training set size: 120\n",
      "Test set size: 30\n",
      "Starting training...\n",
      "Epoch [10/50], Train Loss: 0.7137, Test Loss: 0.6310\n",
      "Epoch [20/50], Train Loss: 0.3657, Test Loss: 0.2785\n",
      "Epoch [30/50], Train Loss: 0.2640, Test Loss: 0.1606\n",
      "Epoch [40/50], Train Loss: 0.1900, Test Loss: 0.1034\n",
      "Epoch [50/50], Train Loss: 0.1588, Test Loss: 0.0696\n",
      "Training completed!\n",
      "Final Test Accuracy:  1.0\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "!python intro-mlops-1/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64f357c-7043-4093-8b5d-e45413b3c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# src/trainer.py\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from .metrics import calculate_accuracy\n",
      "torch.manual_seed(42)\n",
      "\n",
      "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
      "    \"\"\"Helper function to complete one training epoch and return average train loss\"\"\"\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch_X, batch_y in train_loader:\n",
      "        optimizer.zero_grad()\n",
      "        outputs = model(batch_X)\n",
      "        loss = criterion(outputs, batch_y)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        total_loss += loss.item()\n",
      "    avg_train_loss = total_loss / len(train_loader)\n",
      "\n",
      "    return avg_train_loss\n",
      "\n",
      "def evaluate_model(model, criterion, X_test, y_test):\n",
      "    \"\"\"Helper function to evaluate the model after each training epoch\"\"\"\n",
      "    model.eval()\n",
      "    with torch.no_grad():\n",
      "        test_outputs = model(X_test)\n",
      "        test_loss = criterion(test_outputs, y_test)\n",
      "        _, predicted = torch.max(test_outputs, 1)\n",
      "        # Calculate Accuracy\n",
      "        accuracy = calculate_accuracy(predicted, y_test)\n",
      "    \n",
      "    return accuracy, test_loss\n",
      "\n",
      "\n",
      "def train_model(model, train_loader, X_test, y_test, criterion, optimizer, epochs=50):\n",
      "    \"\"\"Higher level training driver function, returns metric arrays\"\"\"\n",
      "    print(\"Starting training...\")\n",
      "    train_losses = [] # these are for metric tracking, these will be returned from train_model()\n",
      "    test_losses = []\n",
      "    accuracies = []\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        #training one epoch\n",
      "        avg_train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
      "\n",
      "        #appending returned value from train_one_epoch to train_losses list\n",
      "        train_losses.append(avg_train_loss)\n",
      "\n",
      "        #evaluating model using function \n",
      "        accuracy, test_loss = evaluate_model(model, criterion, X_test, y_test)\n",
      "\n",
      "        #appeneding returned values from evaluate_model to list\n",
      "        test_losses.append(test_loss.item())\n",
      "        accuracies.append(accuracy)\n",
      "\n",
      "        #print results\n",
      "        if (epoch + 1) % 10 == 0:\n",
      "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {test_loss.item():.4f}')\n",
      "\n",
      "    print(\"Training completed!\")\n",
      "\n",
      "    return train_losses, test_losses, accuracies"
     ]
    }
   ],
   "source": [
    "!cat intro-mlops-1/src/trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f29c-d245-4d2b-9fc1-ca14cb6087d9",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cffc767-d1c8-4d64-b7dc-f0d2ee8a80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (150, 5)\n",
      "Preprocessing data...\n",
      "Training set size: 120\n",
      "Test set size: 30\n",
      "Starting training...\n",
      "Epoch [10/50], Train Loss: 0.7137, Test Loss: 0.6310\n",
      "Epoch [20/50], Train Loss: 0.3657, Test Loss: 0.2785\n",
      "Epoch [30/50], Train Loss: 0.2640, Test Loss: 0.1606\n",
      "Epoch [40/50], Train Loss: 0.1900, Test Loss: 0.1034\n",
      "Epoch [50/50], Train Loss: 0.1588, Test Loss: 0.0696\n",
      "Training completed!\n",
      "Final Test Accuracy:  1.0\n",
      "Model saved to models/best_model.pth\n",
      "Label encoder saved to models/label_encoder.pkl\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#rerunning the pipeline with the models directory added\n",
    "!python intro-mlops-1/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9719a1-9ff7-4cad-bf46-ae551a12b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro-mlops-1/\n",
      "    app.py\n",
      "    README.md\n",
      "    requirements.txt\n",
      "    main.py\n",
      "    plots/\n",
      "        accuracy_plot.png\n",
      "        training_loss_plot.png\n",
      "    notebooks/\n",
      "        analysis.ipynb\n",
      "    logs/\n",
      "        error.log\n",
      "        debug.log\n",
      "        results.log\n",
      "        training.log\n",
      "    models/\n",
      "        label_encoder.pkl\n",
      "        best_model.pth\n",
      "    data/\n",
      "        data.csv\n",
      "    src/\n",
      "        data_loader.py\n",
      "        __init__.py\n",
      "        visualization.py\n",
      "        neural_net.py\n",
      "        trainer.py\n",
      "        metrics.py\n"
     ]
    }
   ],
   "source": [
    "print_directory_tree(root_path, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9cdac-3e92-498f-83fa-e089bfc44ac8",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d370d7c9-06db-42b9-b75f-240481a5c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'versicolor'}\n"
     ]
    }
   ],
   "source": [
    "# sending a request to the server\n",
    "import requests\n",
    "\n",
    "url = \"http://0.0.0.0:8000/predict\"\n",
    "data = [5.1, 3.5, 1.4, 0.2]\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "prediction = response.json()\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae0f898a-bfad-4ad4-a04e-deeab4ff1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro-mlops-1/\n",
      "    app.py\n",
      "    README.md\n",
      "    requirements.txt\n",
      "    main.py\n",
      "    plots/\n",
      "        accuracy_plot.png\n",
      "        training_loss_plot.png\n",
      "    notebooks/\n",
      "        analysis.ipynb\n",
      "    logs/\n",
      "        error.log\n",
      "        debug.log\n",
      "        results.log\n",
      "        training.log\n",
      "    models/\n",
      "        label_encoder.pkl\n",
      "        best_model.pth\n",
      "    data/\n",
      "        data.csv\n",
      "    src/\n",
      "        data_loader.py\n",
      "        __init__.py\n",
      "        visualization.py\n",
      "        neural_net.py\n",
      "        trainer.py\n",
      "        metrics.py\n"
     ]
    }
   ],
   "source": [
    "#displaying the file structure one last time\n",
    "print_directory_tree(root_path, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87042402-b6e6-4450-b63e-ded309333382",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ea433ab-7593-4876-b21b-7c43e7cd9c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pathlib import Path\n",
      "\n",
      "#Directory paths\n",
      "ROOT_PATH = Path(__file__).parent.parent\n",
      "\n",
      "DATA_PATH = ROOT_PATH / 'data'\n",
      "LOGS_PATH = ROOT_PATH / 'logs'\n",
      "MODELS_PATH = ROOT_PATH / 'models'\n",
      "NOTEBOOKS_PATH = ROOT_PATH / 'notebooks'\n",
      "PLOTS_PATH = ROOT_PATH / 'plots'\n",
      "\n",
      "# Model parameters\n",
      "TRAIN_SPLIT = 0.8\n",
      "BATCH_SIZE = 32\n",
      "LEARNING_RATE = 0.001\n",
      "EPOCHS = 50\n",
      "INPUT_SIZE = 4\n",
      "NUM_CLASSES = 3"
     ]
    }
   ],
   "source": [
    "# checking to ensure updated configuration is correct\n",
    "!cat intro-mlops-1/src/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2663cdb7-aadf-4d0a-9b25-090363366bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from pathlib import Path\n",
      "\n",
      "from src.data_loader import load_and_preprocess_data, split_data, create_data_loaders\n",
      "from src.neural_net import SimpleNN\n",
      "from src.trainer import train_model\n",
      "from src.visualization import plot_training_loss, plot_accuracy\n",
      "from src.config import DATA_PATH, LOGS_PATH, MODELS_PATH, PLOTS_PATH, TRAIN_SPLIT, BATCH_SIZE, LEARNING_RATE, EPOCHS, INPUT_SIZE, NUM_CLASSES\n",
      "\n",
      "# set seed\n",
      "torch.manual_seed(42)\n",
      "np.random.seed(42)\n",
      "random.seed(42)\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Using Path objects makes it really easy to modify paths when we refactor - look up pathlib docs if you want to learn more!\n",
      "    Normally we would put these in a global file and import them but we will not do that for this project (but feel free too!)\n",
      "    \"\"\"\n",
      "    #load and preprocess data\n",
      "    X,y, label_encoder = load_and_preprocess_data(DATA_PATH/\"data.csv\")\n",
      "    \n",
      "    #splitting data\n",
      "    X_train, X_test, y_train, y_test = split_data(X,y, TRAIN_SPLIT)\n",
      "\n",
      "    #creating data loaders\n",
      "    train_loader = create_data_loaders(X_train, y_train, BATCH_SIZE)\n",
      "\n",
      "    #instantiating the model\n",
      "    # Initialize model, loss function, optimizer    \n",
      "    model = SimpleNN(INPUT_SIZE, NUM_CLASSES)\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "    #training the model\n",
      "    train_losses, test_losses, accuracies = train_model(model, train_loader, X_test, y_test, criterion, optimizer, epochs=EPOCHS)\n",
      "\n",
      "    #plotting the metrics of the model\n",
      "\n",
      "    # Save Training/Test Loss Plot\n",
      "    plot_training_loss(train_losses,test_losses, PLOTS_PATH) \n",
      "    \n",
      "    # Save Accuracy Plot\n",
      "    plot_accuracy(accuracies, PLOTS_PATH)\n",
      "\n",
      "    #saving results\n",
      "    #Save some results to a file\n",
      "    accuracy = accuracies[-1]\n",
      "    print(\"Final Test Accuracy: \", round(accuracy, 4))\n",
      "    with open(LOGS_PATH / 'results.log', 'w') as f:\n",
      "        f.write(f\"Final Test Accuracy: {accuracy}\\n\")\n",
      "        f.write(f\"Training epochs: {EPOCHS}\\n\")\n",
      "        f.write(f\"Model architecture: SimpleNN with {INPUT_SIZE} input features\\n\")\n",
      "\n",
      "    # Save model weights\n",
      "    torch.save(model.state_dict(), MODELS_PATH / \"best_model.pth\")\n",
      "    print(\"Model saved to models/best_model.pth\")\n",
      "    \n",
      "    # Save the label encoder\n",
      "    import pickle\n",
      "    with open(MODELS_PATH / \"label_encoder.pkl\", \"wb\") as f:\n",
      "      pickle.dump(label_encoder, f)\n",
      "    print(\"Label encoder saved to models/label_encoder.pkl\")\n",
      "\n",
      "    print(\"All done!\") \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ]
    }
   ],
   "source": [
    "!cat intro-mlops-1/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aebab3b2-e314-412f-a165-d8657a9286db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fastapi import FastAPI\n",
      "import torch\n",
      "import pickle\n",
      "\n",
      "from src.neural_net import SimpleNN\n",
      "from src.config import MODELS_PATH, INPUT_SIZE, NUM_CLASSES\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# loading the model up\n",
      "model = SimpleNN(INPUT_SIZE, NUM_CLASSES) # recreating the model architecture!\n",
      "model.load_state_dict(torch.load(MODELS_PATH / \"best_model.pth\"))\n",
      "model.eval()\n",
      "\n",
      "with open(MODELS_PATH / \"label_encoder.pkl\", \"rb\") as f:\n",
      "    label_encoder = pickle.load(f)\n",
      "\n",
      "@app.post(\"/predict\")\n",
      "def predict(data: list[float]):\n",
      "    features = torch.FloatTensor(data).unsqueeze(0)\n",
      "\n",
      "    with torch.no_grad():\n",
      "      outputs = model(features)\n",
      "      _, predicted_idx = torch.max(outputs, 1)\n",
      "      predicted_label = label_encoder.inverse_transform(predicted_idx.numpy())[0] # decodes the encoded label\n",
      "\n",
      "    return {\"prediction\": predicted_label}"
     ]
    }
   ],
   "source": [
    "!cat intro-mlops-1/app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "881ba4c1-c205-40fe-a74d-7f558c20b7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (150, 5)\n",
      "Preprocessing data...\n",
      "Training set size: 120\n",
      "Test set size: 30\n",
      "Starting training...\n",
      "Epoch [10/50], Train Loss: 0.7137, Test Loss: 0.6310\n",
      "Epoch [20/50], Train Loss: 0.3657, Test Loss: 0.2785\n",
      "Epoch [30/50], Train Loss: 0.2640, Test Loss: 0.1606\n",
      "Epoch [40/50], Train Loss: 0.1900, Test Loss: 0.1034\n",
      "Epoch [50/50], Train Loss: 0.1588, Test Loss: 0.0696\n",
      "Training completed!\n",
      "Final Test Accuracy:  1.0\n",
      "Model saved to models/best_model.pth\n",
      "Label encoder saved to models/label_encoder.pkl\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#running pipeline to ensure it is functioning correctly\n",
    "!python intro-mlops-1/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d1ff816-2adc-4328-8449-46c150da45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'virginica'}\n"
     ]
    }
   ],
   "source": [
    "#making a post request to ensure the api is still functioning\n",
    "\n",
    "url = \"http://0.0.0.0:8000/predict\"\n",
    "data = [4.3, 2.6, 1.2, 1.7]\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "prediction = response.json()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76442d6-d02e-4f26-b9d6-c3183e1d6929",
   "metadata": {},
   "source": [
    "## Pledge\n",
    "\n",
    "By submitting this work I hereby pledge that this is my own, personal work. I've acknowledged in the designated place at the top of this file all sources that I used to complete said work, including but not limited to: online resources, books, and electronic communications. I've noted all collaboration with fellow students and/or TA's. I did not copy or plagiarize another's work.\n",
    "\n",
    "> As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do. Accountable together â€“ We are Purdue.\n",
    "\n",
    "https://www.purdue.edu/odos/osrr/honor-pledge/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
