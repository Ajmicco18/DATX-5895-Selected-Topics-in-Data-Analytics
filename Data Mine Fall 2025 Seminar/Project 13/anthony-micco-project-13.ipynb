{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be02a957-7133-4d02-818e-fedeb3cecb05",
   "metadata": {},
   "source": [
    "# Project 13 -- Anthony Micco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1228853-dd19-4ab2-89e0-0394d7d72de3",
   "metadata": {},
   "source": [
    "**TA Help:** N/A\n",
    "\n",
    "**Collaboration:** N/A\n",
    "\n",
    "**Internet Resources:** Used Stack Overflow to help me determine I needed to use HAVING instead of WHERE when working with already aggregated data (https://stackoverflow.com/questions/42470849/why-are-aggregate-functions-not-allowed-in-where-clause)\n",
    "\n",
    "**ChatGPT, Gemini, Claude, etc:** None\n",
    "\n",
    "**Link to AI Chat History**: None\n",
    "\n",
    "**OVERALL MESSAGE:** Any time that you used anything except your brain to solve the questions in these projects, you need to disclose such resources at the start of the project, with details about your usage of the tools.\n",
    "\n",
    "**YOUR OWN WORK:** Even when you utilize other resources, do NOT just copy and paste.  Write all explanations in your own words, using several sentences in English, which are understandable and which you wrote (and did not just copy and paste)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180e742-8e39-4698-98ff-5b00c8cf8ea0",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49445606-d363-41b4-b479-e319a9a84c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to initialize a SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"TDM_S\").config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "# Initialize a SparkSession called 'TDM_S' with 2GB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "997a7165-5167-417d-a230-88f5bf046f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the weather.parquet dataset \n",
    "df = spark.read.parquet(\"/anvil/projects/tdm/data/whin/weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02cf2e92-8df2-423e-9532-cec291e37046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "|station_id|latitude|longitude|           name|    observation_time|temperature|temperature_high|temperature_low|humidity|solar_radiation|solar_radiation_high|rain|rain_inches_last_hour|wind_speed_mph|wind_direction_degrees|wind_gust_speed_mph|wind_gust_direction_degrees|pressure|soil_temp_1|soil_temp_2|soil_temp_3|soil_temp_4|soil_moist_1|soil_moist_2|soil_moist_3|soil_moist_4|\n",
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-10T04:00:00Z|       70.0|            71.0|           70.0|    83.0|           NULL|                NULL| 0.0|                  0.0|           0.0|                  NULL|                3.0|                      247.5|   30.05|       77.0|       78.0|       76.0|       74.0|        24.0|        24.0|        10.0|         9.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-10T04:15:00Z|       69.0|            70.0|           69.0|    84.0|           NULL|                NULL| 0.0|                  0.0|           1.0|                 247.5|                3.0|                      247.5|   30.04|       76.0|       78.0|       76.0|       74.0|        24.0|        25.0|        10.0|         9.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:00:00Z|       76.0|            77.0|           76.0|    76.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 202.5|                4.0|                      202.5|   29.89|       80.0|       80.0|       78.0|       75.0|        31.0|        30.0|        12.0|        10.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:15:00Z|       76.0|            76.0|           76.0|    77.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 202.5|                4.0|                      202.5|   29.88|       80.0|       80.0|       78.0|       75.0|        31.0|        31.0|        12.0|        10.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:30:00Z|       76.0|            76.0|           76.0|    77.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 225.0|                4.0|                      202.5|   29.88|       80.0|       80.0|       78.0|       75.0|        32.0|        31.0|        12.0|        10.0|\n",
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#showing the first 5 lines of the dataset\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc601975-35ed-4680-a4e1-0273ee3cc047",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a16336a1-1ef0-41e8-bc7c-49387db27497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- observation_time: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- temperature_high: double (nullable = true)\n",
      " |-- temperature_low: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- solar_radiation: double (nullable = true)\n",
      " |-- solar_radiation_high: double (nullable = true)\n",
      " |-- rain: double (nullable = true)\n",
      " |-- rain_inches_last_hour: double (nullable = true)\n",
      " |-- wind_speed_mph: double (nullable = true)\n",
      " |-- wind_direction_degrees: double (nullable = true)\n",
      " |-- wind_gust_speed_mph: double (nullable = true)\n",
      " |-- wind_gust_direction_degrees: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- soil_temp_1: double (nullable = true)\n",
      " |-- soil_temp_2: double (nullable = true)\n",
      " |-- soil_temp_3: double (nullable = true)\n",
      " |-- soil_temp_4: double (nullable = true)\n",
      " |-- soil_moist_1: double (nullable = true)\n",
      " |-- soil_moist_2: double (nullable = true)\n",
      " |-- soil_moist_3: double (nullable = true)\n",
      " |-- soil_moist_4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing out basic info about the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "211fa5d0-267e-409e-8227-578c9a7f89ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+---------------+\n",
      "|station_id|average_wind_speed_mph|max_temperature|\n",
      "+----------+----------------------+---------------+\n",
      "|       167|     8.294951993017166|           93.0|\n",
      "|        85|     7.560077632217371|           94.0|\n",
      "|       145|     7.392609344809415|           95.0|\n",
      "|       109|      7.25007006071929|           93.0|\n",
      "|        81|     7.246332637648083|          158.0|\n",
      "+----------+----------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#using the example to test the function of PySpark\n",
    "test_df = df.groupBy(\"station_id\").agg(avg(\"wind_speed_mph\").alias(\"average_wind_speed_mph\"), max(\"temperature\").alias(\"max_temperature\")).filter(col(\"max_temperature\") > 80).sort(desc(\"average_wind_speed_mph\"))\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da16b53a-cf24-428f-b015-1c11db4a1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.026845216751098633 seconds\n"
     ]
    }
   ],
   "source": [
    "# operations to aggregate, filter, sort and group the data\n",
    "import time\n",
    "start_time = time.time()\n",
    "result_df = df.groupBy(\"name\").agg(avg(\"humidity\").alias(\"avg_humidity\"),avg(\"solar_radiation\").alias(\"avg_solar_radiation\"),avg(\"pressure\").alias(\"avg_pressure\")).filter((col(\"avg_humidity\") > 50) &  (col(\"avg_solar_radiation\") > 100)).sort(desc(\"avg_pressure\"))\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be0fe830-5495-451f-b2e4-210b3972a16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-------------------+------------------+\n",
      "|            name|     avg_humidity|avg_solar_radiation|      avg_pressure|\n",
      "+----------------+-----------------+-------------------+------------------+\n",
      "|WHIN089E-BENT009|74.77029578351164| 257.34197608558844|30.216278791692908|\n",
      "|      Bringhurst|80.17502311929556| 168.08889779559118|30.190432274525733|\n",
      "| WHIN020-FOUN001|76.72776597425857| 146.12622262805766|30.159863727349002|\n",
      "|        Idaville|77.37444567283207| 166.06921923967843|30.151303970200168|\n",
      "|         Wolcott|67.00495091507085| 104.93433155080214| 30.11409279198035|\n",
      "+----------------+-----------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "Time taken: 1.964643955230713 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#printing out the first five rows of that data set\n",
    "start_time = time.time()\n",
    "result_df.show(5)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586edd-ff26-4ce2-8f6b-2424b26f2929",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbe0f40d-9655-4653-9ca8-886bdb61cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary view of the dataframe\n",
    "df.createOrReplaceTempView(\"weather_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6459b234-1a28-4157-af0b-5311f369177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing an SQL query using PySpark function\n",
    "test_df = spark.sql(\"SELECT AVG(humidity), AVG(solar_radiation), AVG(pressure) FROM weather_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13c6f144-5c7d-4733-b4df-cc177fc36d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+------------------+\n",
      "|   avg(humidity)|avg(solar_radiation)|     avg(pressure)|\n",
      "+----------------+--------------------+------------------+\n",
      "|77.5701237585927|  183.93572113847137|29.912132181425918|\n",
      "+----------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing the first five rows\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20921edd-0ecd-4e11-b536-cdfa5b50cd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.012638092041015625 seconds\n"
     ]
    }
   ],
   "source": [
    "#using SQL with PySpark to find the same operations as the previous question\n",
    "start_time = time.time()\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT name, \n",
    "AVG(humidity) AS avg_humidity,\n",
    "AVG(solar_radiation) AS avg_solar_radiation,\n",
    "AVG(pressure) AS avg_pressure\n",
    "FROM weather_view\n",
    "GROUP BY name\n",
    "HAVING (avg_humidity > 50) AND (avg_solar_radiation > 100)\n",
    "ORDER BY avg_pressure DESC\n",
    "\"\"\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86e0a472-2ed4-44f6-8e7c-250222febaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-------------------+------------------+\n",
      "|            name|     avg_humidity|avg_solar_radiation|      avg_pressure|\n",
      "+----------------+-----------------+-------------------+------------------+\n",
      "|WHIN089E-BENT009|74.77029578351164| 257.34197608558844|30.216278791692908|\n",
      "|      Bringhurst|80.17502311929556| 168.08889779559118|30.190432274525733|\n",
      "| WHIN020-FOUN001|76.72776597425857| 146.12622262805766|30.159863727349002|\n",
      "|        Idaville|77.37444567283207| 166.06921923967843|30.151303970200168|\n",
      "|         Wolcott|67.00495091507085| 104.93433155080214| 30.11409279198035|\n",
      "+----------------+-----------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "Time taken: 0.953681230545044 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#determing how much time it takes to run\n",
    "start_time = time.time()\n",
    "result_df.show(5)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6229f-35f7-400c-8366-c442baa5cf47",
   "metadata": {},
   "source": [
    "3.4) It takes 0.953681230545044 seconds to print the first 5 rows of the result DataFrame \\\n",
    "3.5) It is a little over a second faster to print the first 5 rows of the result using PySpark SQL when compared to using the traditional PySpark DataFrame API. It took PySpark SQL only 0.953681230545044 seconds, while the PySpark DataFrame API took 1.964643955230713 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f29c-d245-4d2b-9fc1-ca14cb6087d9",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cffc767-d1c8-4d64-b7dc-f0d2ee8a80d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-18 15:35:30.010\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 2 in cell [43]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o283.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703;\\n'Project [station_id#392L, latitude#393, longitude#394, name#395, observation_time#396, temperature#397, temperature_high#398, temperature_low#399, humidity#400, solar_radiation#401, solar_radiation_high#402, rain#403, rain_inches_last_hour#404, wind_speed_mph#405, wind_direction_degrees#406, wind_gust_speed_mph#407, wind_gust_direction_degrees#408, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, ... 2 more fields]\\n+- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703;\n'Project [station_id#392L, latitude#393, longitude#394, name#395, observation_time#396, temperature#397, temperature_high#398, temperature_low#399, humidity#400, solar_radiation#401, solar_radiation_high#402, rain#403, rain_inches_last_hour#404, wind_speed_mph#405, wind_direction_degrees#406, wind_gust_speed_mph#407, wind_gust_direction_degrees#408, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, ... 2 more fields]\n+- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# creating a new column where every value in the new column is one more than the value in the old column\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m newdf = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnew_column\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mold_column\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1623\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1620\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1621\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1622\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703;\n'Project [station_id#392L, latitude#393, longitude#394, name#395, observation_time#396, temperature#397, temperature_high#398, temperature_low#399, humidity#400, solar_radiation#401, solar_radiation_high#402, rain#403, rain_inches_last_hour#404, wind_speed_mph#405, wind_direction_degrees#406, wind_gust_speed_mph#407, wind_gust_direction_degrees#408, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, ... 2 more fields]\n+- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet\n"
     ]
    }
   ],
   "source": [
    "#how to create a new column where every value in the new column is one more than the value in the old column\n",
    "newdf = df.withColumn(\"new_column\", col(\"old_column\") + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a69886b-1f75-46de-9034-76e63171213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to remove a column from a dataframe\n",
    "newdf = df.drop(\"column_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7924a67a-c1f3-4e7b-80bb-dae781caa578",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-18 15:37:25.698\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 2 in cell [46]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o283.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703;\\n'Project [station_id#392L, latitude#393, longitude#394, name#395, observation_time#396, temperature#397, temperature_high#398, temperature_low#399, humidity#400, solar_radiation#401, solar_radiation_high#402, rain#403, rain_inches_last_hour#404, wind_speed_mph#405, wind_direction_degrees#406, wind_gust_speed_mph#407, wind_gust_direction_degrees#408, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, ... 2 more fields]\\n+- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703;\n'Project [station_id#392L, latitude#393, longitude#394, name#395, observation_time#396, temperature#397, temperature_high#398, temperature_low#399, humidity#400, solar_radiation#401, solar_radiation_high#402, rain#403, rain_inches_last_hour#404, wind_speed_mph#405, wind_direction_degrees#406, wind_gust_speed_mph#407, wind_gust_direction_degrees#408, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, ... 2 more fields]\n+- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#how to change the data type of a column by either creating a new column or overwriting an existing one\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m newdf = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnew_column\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mold_column\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstring\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m newdf = df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mold_column\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mold_column\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1623\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1620\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1621\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1622\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `old_column` cannot be resolved. Did you mean one of the following? [`latitude`, `longitude`, `humidity`, `name`, `pressure`]. SQLSTATE: 42703;\n'Project [station_id#392L, latitude#393, longitude#394, name#395, observation_time#396, temperature#397, temperature_high#398, temperature_low#399, humidity#400, solar_radiation#401, solar_radiation_high#402, rain#403, rain_inches_last_hour#404, wind_speed_mph#405, wind_direction_degrees#406, wind_gust_speed_mph#407, wind_gust_direction_degrees#408, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, ... 2 more fields]\n+- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet\n"
     ]
    }
   ],
   "source": [
    "#how to change the data type of a column by either creating a new column or overwriting an existing one\n",
    "newdf = df.withColumn(\"new_column\", col(\"old_column\").cast(\"string\"))\n",
    "\n",
    "newdf = df.withColumn(\"old_column\", col(\"old_column\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff0863a1-995f-4513-82cb-b0a1d2e37ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the observation_time column to a timestamp type\n",
    "newdf = df.withColumn(\"observation_time\", to_timestamp(\"observation_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cbe39b2-f044-4e21-9955-e6de30f4beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the year of the timestamp in a new column\n",
    "newdf = newdf.withColumn(\"year\", year(\"observation_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6a5f395-c564-4730-8104-fc79afff2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the month of the timestamp in a new column\n",
    "newdf = newdf.withColumn(\"month\", month(\"observation_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d068e435-1ed0-4003-b764-b2f25208521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- observation_time: timestamp (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- temperature_high: double (nullable = true)\n",
      " |-- temperature_low: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- solar_radiation: double (nullable = true)\n",
      " |-- solar_radiation_high: double (nullable = true)\n",
      " |-- rain: double (nullable = true)\n",
      " |-- rain_inches_last_hour: double (nullable = true)\n",
      " |-- wind_speed_mph: double (nullable = true)\n",
      " |-- wind_direction_degrees: double (nullable = true)\n",
      " |-- wind_gust_speed_mph: double (nullable = true)\n",
      " |-- wind_gust_direction_degrees: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- soil_temp_1: double (nullable = true)\n",
      " |-- soil_temp_2: double (nullable = true)\n",
      " |-- soil_temp_3: double (nullable = true)\n",
      " |-- soil_temp_4: double (nullable = true)\n",
      " |-- soil_moist_1: double (nullable = true)\n",
      " |-- soil_moist_2: double (nullable = true)\n",
      " |-- soil_moist_3: double (nullable = true)\n",
      " |-- soil_moist_4: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying the new columns\n",
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f117750b-3582-4d10-9303-7f3afaaee432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping the data by the month and then aggregating the temperature and solar_radiation by the average\n",
    "\n",
    "#creating a new view for the new dataframe we created\n",
    "newdf.createOrReplaceTempView(\"date_view\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    month,\n",
    "    AVG(temperature) AS avg_temp,\n",
    "    AVG(solar_radiation) AS avg_solar_radiation\n",
    "    FROM date_view\n",
    "    GROUP BY month\n",
    "    ORDER BY avg_temp DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c882052-f8b2-416d-b4e4-ddcf5d4e3640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-------------------+\n",
      "|month|         avg_temp|avg_solar_radiation|\n",
      "+-----+-----------------+-------------------+\n",
      "|    7| 73.9766158271431| 253.92342957199992|\n",
      "|    6|73.41862618893836| 267.88055625964927|\n",
      "|    8|71.00368368101219| 253.58956703962878|\n",
      "|    9|66.70186356073211|  179.6384488924556|\n",
      "|    5|60.18404719420269|  232.1310546777555|\n",
      "+-----+-----------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#printing the top 5 results\n",
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9cdac-3e92-498f-83fa-e089bfc44ac8",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d370d7c9-06db-42b9-b75f-240481a5c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_temp#854 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_temp#854 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=420]\n",
      "      +- HashAggregate(keys=[month#853], functions=[avg(temperature#397), avg(solar_radiation#401)])\n",
      "         +- Exchange hashpartitioning(month#853, 200), ENSURE_REQUIREMENTS, [plan_id=417]\n",
      "            +- HashAggregate(keys=[month#853], functions=[partial_avg(temperature#397), partial_avg(solar_radiation#401)])\n",
      "               +- Project [temperature#397, solar_radiation#401, month(cast(cast(observation_time#396 as timestamp) as date)) AS month#853]\n",
      "                  +- FileScan parquet [observation_time#396,temperature#397,solar_radiation#401] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/anvil/projects/tdm/data/whin/weather.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<observation_time:string,temperature:double,solar_radiation:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the explain method on code from question 4\n",
    "result_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1cc7f36-8716-4738-9ed9-3ef6c83e8f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Sort [avg_temp#854 DESC NULLS LAST], true, Statistics(sizeInBytes=9.9 MiB)\n",
      "+- Aggregate [month#853], [month#853, avg(temperature#397) AS avg_temp#854, avg(solar_radiation#401) AS avg_solar_radiation#855], Statistics(sizeInBytes=9.9 MiB)\n",
      "   +- Project [temperature#397, solar_radiation#401, month(cast(cast(observation_time#396 as timestamp) as date)) AS month#853], Statistics(sizeInBytes=9.9 MiB)\n",
      "      +- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet, Statistics(sizeInBytes=85.1 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_temp#854 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_temp#854 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=420]\n",
      "      +- HashAggregate(keys=[month#853], functions=[avg(temperature#397), avg(solar_radiation#401)], output=[month#853, avg_temp#854, avg_solar_radiation#855])\n",
      "         +- Exchange hashpartitioning(month#853, 200), ENSURE_REQUIREMENTS, [plan_id=417]\n",
      "            +- HashAggregate(keys=[month#853], functions=[partial_avg(temperature#397), partial_avg(solar_radiation#401)], output=[month#853, sum#865, count#866L, sum#867, count#868L])\n",
      "               +- Project [temperature#397, solar_radiation#401, month(cast(cast(observation_time#396 as timestamp) as date)) AS month#853]\n",
      "                  +- FileScan parquet [observation_time#396,temperature#397,solar_radiation#401] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/anvil/projects/tdm/data/whin/weather.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<observation_time:string,temperature:double,solar_radiation:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using mode=cost to print out the Optimized Logical Plan\n",
    "result_df.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71ea7e9e-0816-46ee-9431-434de7aea764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a long, complex SQL query that has many steps\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT station_id, \n",
    "AVG(humidity) AS avg_humidity,\n",
    "AVG(solar_radiation) AS avg_solar_radiation,\n",
    "AVG(pressure) AS avg_pressure,\n",
    "MAX(rain) AS max_rain,\n",
    "AVG(soil_temp_1+soil_temp_2+soil_temp_3+soil_temp_4) AS avg_soil_temp,\n",
    "MIN(soil_moist_1+soil_moist_2+soil_moist_3+soil_moist_4) AS min_soil_moist\n",
    "FROM weather_view\n",
    "GROUP BY station_id\n",
    "HAVING (avg_humidity > 50) AND (avg_solar_radiation > 100)\n",
    "ORDER BY avg_pressure DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6ecf191-3eee-41fa-a4b5-e6fddd0b8879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Sort [avg_pressure#927 DESC NULLS LAST], true, Statistics(sizeInBytes=22.7 MiB)\n",
      "+- Filter ((isnotnull(avg_humidity#925) AND isnotnull(avg_solar_radiation#926)) AND ((avg_humidity#925 > 50.0) AND (avg_solar_radiation#926 > 100.0))), Statistics(sizeInBytes=22.7 MiB)\n",
      "   +- Aggregate [station_id#392L], [station_id#392L, avg(humidity#400) AS avg_humidity#925, avg(solar_radiation#401) AS avg_solar_radiation#926, avg(pressure#409) AS avg_pressure#927, max(rain#403) AS max_rain#928, avg((((soil_temp_1#410 + soil_temp_2#411) + soil_temp_3#412) + soil_temp_4#413)) AS avg_soil_temp#929, min((((soil_moist_1#414 + soil_moist_2#415) + soil_moist_3#416) + soil_moist_4#417)) AS min_soil_moist#930], Statistics(sizeInBytes=22.7 MiB)\n",
      "      +- Project [station_id#392L, humidity#400, solar_radiation#401, rain#403, pressure#409, soil_temp_1#410, soil_temp_2#411, soil_temp_3#412, soil_temp_4#413, soil_moist_1#414, soil_moist_2#415, soil_moist_3#416, soil_moist_4#417], Statistics(sizeInBytes=39.7 MiB)\n",
      "         +- Relation [station_id#392L,latitude#393,longitude#394,name#395,observation_time#396,temperature#397,temperature_high#398,temperature_low#399,humidity#400,solar_radiation#401,solar_radiation_high#402,rain#403,rain_inches_last_hour#404,wind_speed_mph#405,wind_direction_degrees#406,wind_gust_speed_mph#407,wind_gust_direction_degrees#408,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,... 1 more fields] parquet, Statistics(sizeInBytes=85.1 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg_pressure#927 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg_pressure#927 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=445]\n",
      "      +- Filter ((isnotnull(avg_humidity#925) AND isnotnull(avg_solar_radiation#926)) AND ((avg_humidity#925 > 50.0) AND (avg_solar_radiation#926 > 100.0)))\n",
      "         +- HashAggregate(keys=[station_id#392L], functions=[avg(humidity#400), avg(solar_radiation#401), avg(pressure#409), max(rain#403), avg((((soil_temp_1#410 + soil_temp_2#411) + soil_temp_3#412) + soil_temp_4#413)), min((((soil_moist_1#414 + soil_moist_2#415) + soil_moist_3#416) + soil_moist_4#417))], output=[station_id#392L, avg_humidity#925, avg_solar_radiation#926, avg_pressure#927, max_rain#928, avg_soil_temp#929, min_soil_moist#930])\n",
      "            +- Exchange hashpartitioning(station_id#392L, 200), ENSURE_REQUIREMENTS, [plan_id=441]\n",
      "               +- HashAggregate(keys=[station_id#392L], functions=[partial_avg(humidity#400), partial_avg(solar_radiation#401), partial_avg(pressure#409), partial_max(rain#403), partial_avg((((soil_temp_1#410 + soil_temp_2#411) + soil_temp_3#412) + soil_temp_4#413)), partial_min((((soil_moist_1#414 + soil_moist_2#415) + soil_moist_3#416) + soil_moist_4#417))], output=[station_id#392L, sum#947, count#948L, sum#949, count#950L, sum#951, count#952L, max#953, sum#954, count#955L, min#956])\n",
      "                  +- FileScan parquet [station_id#392L,humidity#400,solar_radiation#401,rain#403,pressure#409,soil_temp_1#410,soil_temp_2#411,soil_temp_3#412,soil_temp_4#413,soil_moist_1#414,soil_moist_2#415,soil_moist_3#416,soil_moist_4#417] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/anvil/projects/tdm/data/whin/weather.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<station_id:bigint,humidity:double,solar_radiation:double,rain:double,pressure:double,soil_...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf00fb-2418-460f-ae94-2a32b0c28952",
   "metadata": {},
   "source": [
    "5.2) My physical plan had 8 steps, while my optimized logical plan had 4 steps.\\\n",
    "5.5) My physical plan had 8 steps, whle my optimized logical plan had 6 steps. \\\n",
    "5.6) Yes, it was much easier to understand the optimized logical plan because it used more identifiable language used in the SQL or API query such as sort and aggregate. It also made it easier to see which columns it was using for the query and did not include as much extra information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76442d6-d02e-4f26-b9d6-c3183e1d6929",
   "metadata": {},
   "source": [
    "## Pledge\n",
    "\n",
    "By submitting this work I hereby pledge that this is my own, personal work. I've acknowledged in the designated place at the top of this file all sources that I used to complete said work, including but not limited to: online resources, books, and electronic communications. I've noted all collaboration with fellow students and/or TA's. I did not copy or plagiarize another's work.\n",
    "\n",
    "> As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do. Accountable together  We are Purdue.\n",
    "\n",
    "https://www.purdue.edu/odos/osrr/honor-pledge/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
